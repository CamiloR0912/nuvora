import json
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

# Intentar cargar LangChain con Ollama (opcional)
USE_LLM = False
try:
    from langchain_ollama import OllamaLLM
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    
    llm = OllamaLLM(model="llama3", base_url="http://ollama:11434")
    
    template = """
Eres un asistente para un sistema de parqueo inteligente llamado Nuvora.
Convierte la pregunta del usuario en una instrucciÃ³n estructurada en formato JSON.

Ejemplos:
Usuario: "Â¿CuÃ¡ntos carros hay en total?"
Respuesta: {{"query_type": "total_cars"}}

Usuario: "Â¿CuÃ¡ntos vehÃ­culos activos hay?"
Respuesta: {{"query_type": "active_vehicles"}}

Usuario: "Buscar placa ABC123"
Respuesta: {{"query_type": "search_plate", "plate": "ABC123"}}

Usuario: "Mostrar historial"
Respuesta: {{"query_type": "history"}}

Usuario: "EstadÃ­sticas del dÃ­a"
Respuesta: {{"query_type": "daily_stats"}}

Usuario: "{user_input}"
Respuesta:"""

    prompt = PromptTemplate(template=template, input_variables=["user_input"])
    chain = prompt | llm
    USE_LLM = True
    logger.info("âœ… LangChain con Ollama configurado")
    
except Exception as e:
    logger.warning(f"âš ï¸ No se pudo cargar Ollama, usando interpretaciÃ³n basada en reglas: {e}")

def interpret_with_llm(user_input: str) -> Dict[str, Any]:
    """Interpreta el comando usando LLM (Ollama)"""
    try:
        response = chain.invoke({"user_input": user_input})
        return json.loads(response)
    except Exception as e:
        logger.error(f"Error con LLM: {e}")
        return interpret_with_rules(user_input)

def interpret_with_rules(user_input: str) -> Dict[str, Any]:
    """
    Interpreta el comando usando reglas simples (fallback).
    Esto funciona sin necesidad de LLM externo.
    """
    text = user_input.lower().strip()
    
    # Reglas de interpretaciÃ³n
    if any(word in text for word in ["cuÃ¡ntos", "cuantos", "total", "cantidad"]):
        if any(word in text for word in ["activos", "parqueadero", "estacionados"]):
            return {"query_type": "active_vehicles"}
        if any(word in text for word in ["ingresaron", "entraron", "entrada"]):
            return {"query_type": "entries_count"}
        return {"query_type": "total_cars"}
    
    elif any(word in text for word in ["buscar", "busca", "encontrar", "placa"]):
        # Extraer placa (formato: ABC123 o ABC-123)
        import re
        plate_match = re.search(r'\b([A-Z]{3}[-\s]?\d{3})\b', text.upper())
        if plate_match:
            plate = plate_match.group(1).replace("-", "").replace(" ", "")
            return {"query_type": "search_plate", "plate": plate}
        return {"query_type": "search_plate", "plate": None}
    
    elif any(word in text for word in ["historial", "salidas", "cerrados"]):
        return {"query_type": "history"}
    
    elif any(word in text for word in ["estadÃ­sticas", "estadisticas", "resumen", "dÃ­a"]):
        return {"query_type": "daily_stats"}
    
    elif any(word in text for word in ["cupos", "espacios", "disponibles"]):
        return {"query_type": "available_spaces"}
    
    elif any(word in text for word in ["Ãºltima", "ultima", "reciente", "placa detectada"]):
        return {"query_type": "last_detection"}
    
    else:
        return {"query_type": "unknown"}

def interpret(user_input: str) -> Dict[str, Any]:
    """
    FunciÃ³n principal de interpretaciÃ³n.
    Usa LLM si estÃ¡ disponible, sino usa reglas.
    """
    logger.info(f"ğŸ§  Interpretando: {user_input}")
    
    if USE_LLM:
        result = interpret_with_llm(user_input)
    else:
        result = interpret_with_rules(user_input)
    
    logger.info(f"âœ… InterpretaciÃ³n: {result}")
    return result